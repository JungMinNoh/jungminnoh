{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Do it 7장 여러개를 분류합니다 2 - 다중분류.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JungMinNoh/jungminnoh/blob/master/Do_it_7%EC%9E%A5_%EC%97%AC%EB%9F%AC%EA%B0%9C%EB%A5%BC_%EB%B6%84%EB%A5%98%ED%95%A9%EB%8B%88%EB%8B%A4_2_%EB%8B%A4%EC%A4%91%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u5-PIrLfFBU",
        "colab_type": "text"
      },
      "source": [
        "6장에서는 양성,음성 클래스(2개의 클래스)가 있는 이진 분류(binary classfication)문제를 위한 완전 연결 신경망을 만들어보았습니다.   \n",
        "이번장에서는 여러개의 클래스가 있는 다중분류(multiclass classification)문제를 풀어보겠습니다.   \n",
        "이진 분류 알고리즘과 다중분류 알고리즘은 비슷한점이 많습니다.  \n",
        " 다중분류의 개념을 익힌 다음에 이전 장과 마찬가지로 파이썬을 사용하여 직정 다중분류 알고리즘을 만들고 그런 다음 현재 가장 인기 있는 딥러닝 패키지인 구글의 텐서플로를 알아보고 이를 사용하여 완전 연결 신경망을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TqQwyppfaGI",
        "colab_type": "text"
      },
      "source": [
        "# 07-1 여러개의 이미지를 분류하는 다층 신경망을 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8hRI31qhtu4",
        "colab_type": "text"
      },
      "source": [
        "다중 분류 신경망을 만들기 위해서는 **소프트맥스** 함수와 **크로스엔트로피 손실함수** 라는 새로운 재료를 알아야 합니다.   \n",
        "두 함수는 다중 분류 신경망의 예측 훈련에서 중요한 역할을 합니다.   \n",
        "자동차 분류 문제를 통해 다중 분류를 위한 신경망과 이진 분류를 위한 신경망의 차이점이 무엇인지 알아보겠습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljIUMLtXigkt",
        "colab_type": "text"
      },
      "source": [
        "## 다중 분류 신경망을 알아봅니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VztPH_YnzJzn",
        "colab_type": "text"
      },
      "source": [
        "다음은 이진 분류에서 사용된 다층 신경망의 구조를 나타낸것입니다.    \n",
        "이진 분류를 위해 신경망의 출력층에는 뉴런을 하나만 두었습니다.   \n",
        "출력층의 활성화 값이 0.5보다 크면 양성 클래스, 그렇지 않으면 음성 클래스로 분류 했습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcHuJtm0A2aQ",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://github.com/JungMinNoh/jungminnoh/blob/master/7-1.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKrfUOOX0IBx",
        "colab_type": "text"
      },
      "source": [
        "다중분류는 신경망을 어떻게 구성해야 할까요? 간단하게 마지막 '**출력층**'에 여러개의 뉴런을 놓는 방법을 생각 할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44Ptzoey0UWW",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://github.com/JungMinNoh/jungminnoh/blob/master/7-2.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8gl27pI0ouW",
        "colab_type": "text"
      },
      "source": [
        "이진 분류와 다중 분류를 위한 신경망의 구조를 나타낸 그림을 비교 해 보니 출력층의 개수만 다르고 나머지는 같습니다.   \n",
        "이진 분류는 양성 클래스에 대한 확률 $\\widehat{y}$ 하나만 출력하고  \n",
        " 다중 분류는 각 클래스에 대한 확률 값을 출력합니다.   \n",
        "\n",
        "예를 들어 첫 번째 클래스에 대한 확률값은 $\\widehat{y_{1}}$  \n",
        "두번째 클래스에 대한 확률값은 $\\widehat{y_{2}}$   \n",
        "세번째 클래스에 대한 확률값은 $\\widehat{y_{3}}$ 으로 출력 됩니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETbeF5DW1UGv",
        "colab_type": "text"
      },
      "source": [
        "## 다중 분류의 문제점과 소프트 맥스 함수를 알아봅니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DiY9ODG1stK",
        "colab_type": "text"
      },
      "source": [
        "다음은 자동차,비행기,로켓이미지를 분류하는 다중 분류 신경망의 출력층만을 나타낸것입니다.   \n",
        "왼쪽 출력층의 활성화 값은 [0.9,0.8,0.7] 오른쪽 출력층의 활성화 값은 [0.5,0.2,0.1] 입니다.   \n",
        "이 값들은 앞에서 말했듯이 확률을 의미합니다.  \n",
        " 예를 들어 왼쪽 출력층의 활성화 값 중 자동차 그림에 있는 0.9라는 값은 이 이미지를 90%정도 확신으로 자동차를 예측한다고 해석할 수 있습니다.  \n",
        " 왼쪽과 오른쪽의 신경망은 모두 자동차를 타깃 클래스로 예측하고 있습니다.  \n",
        " 하지만 왼쪽과 오른쪽의 활성화 값을 비교해 보면 미묘한 차이가 있음을 알 수 있습니다.  \n",
        "왼쪽 출력층의 활성화 값은 모두 높으면서 비슷하고 오른쪽 출력층의 활성화 값은 대체로 낮지만 자동차 클래스의 값(확률)이 비교적 높습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka2Ss0FXBxbF",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://github.com/JungMinNoh/jungminnoh/blob/master/7-3.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wSCLmNb4HVL",
        "colab_type": "text"
      },
      "source": [
        "### 활성화 출력의 합이 1이 아니면 비교하기 어렵습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em1dGJGv4O41",
        "colab_type": "text"
      },
      "source": [
        "여기서 잠시 생각해 볼게 있습니다. 왼쪽에 있는 샘플에 대한 자동차 예측 확률(0.9)이 오른쪽에 있는 샘플에 대한 자동차 예측 확률(0.5)보다 높습니다. 하지만 오른쪽의 확률이 더 정확하고 잘 예측한것입니다. 이런 출력층의 활성화 값은 공정하게 비교하기 어렵습니다. 그래서 정규화가 필요 하게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp5lJCpj48jj",
        "colab_type": "text"
      },
      "source": [
        "### 소프트맥스 함수 적용해 출력 강도를 정규화 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axQ3SZ0S5DjN",
        "colab_type": "text"
      },
      "source": [
        "출력층의 출력 강도를 정규화 하는 소프트 맥스 함수를 사용하면 위 문제를 쉽게 해결 할 수 있습니다.   \n",
        "여기서 출력강도를 정규화 한다는 의미는 전체 출력값의 합을 1로 만든 다는 의미 입니다.   \n",
        "이렇게 하면 값들을 확률로 생각 할 수 있습니다.   \n",
        "앞에서 본 다중 분류 신경망 출력층의 출력은 모두 3개였습니다.   \n",
        " 출력이 3개인 다중분류 신경망에 대한 소프트 맥스 함수의 정의는 다음과 같습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfe3nMId5TtD",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://github.com/JungMinNoh/jungminnoh/blob/master/7-5.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIAWqe0jDG9O",
        "colab_type": "text"
      },
      "source": [
        "위의 예제에 소프트 맥스 함수를 적용 하면 어떻게 될까요?   \n",
        "시그모이드 함수의 활성화 값이 [0.9,0.8,0.7] 로 출력된 예제에 소프트 맥스 함수를 적용해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQfmKGPMB78Z",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://github.com/JungMinNoh/jungminnoh/blob/master/7-4.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPdApX_bDo57",
        "colab_type": "text"
      },
      "source": [
        "소프트 맥스 함수에는 출력층에서 계산된 선형 출력 (z1,z2,z3) 이 필요하므로 시그 모이드 함수 공식을 이용해서 이 값들을 구해 보겠습니다.   \n",
        "시그모이드 함수 공식은"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxCPuV5kEErz",
        "colab_type": "text"
      },
      "source": [
        "#$\\widehat{y}=\\frac{1}{1+e^{-z}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFjdo2X9ETC5",
        "colab_type": "text"
      },
      "source": [
        "이므로 z에 대해서 정리하면"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVx1y5mLEXs0",
        "colab_type": "text"
      },
      "source": [
        "#$z=-ln(\\frac{1}{\\widehat{y}}-1)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHTDcagmE8ZF",
        "colab_type": "text"
      },
      "source": [
        "이 됩니다. 이 식을 이용하여 앞 그림의 z 값을 계산 하면 다음과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0taJSoVKEV6W",
        "colab_type": "text"
      },
      "source": [
        "##$z_{1}=-ln(\\frac{1}{0.9}-1)=2.20$ $z_{2}=-ln(\\frac{1}{0.8}-1)=1.39$ $z_{3}=-ln(\\frac{1}{0.7}-1)=0.85$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6_86w4PFzEE",
        "colab_type": "text"
      },
      "source": [
        "이 값을 소프트 맥스 함수에 대입하면 다음과 같은 정규화된 값을 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st4chRBDF4Li",
        "colab_type": "text"
      },
      "source": [
        "##$\\widehat{y_{1}}=\\frac{e^{2.20}}{e^{2.20}+e^{1.39}+e^{0.85}}=0.59$ $\\widehat{y_{2}}=\\frac{e^{1.39}}{e^{2.20}+e^{1.39}+e^{0.85}}=0.26$$\\widehat{y_{3}}=\\frac{e^{0.85}}{e^{2.20}+e^{1.39}+e^{0.85}}=0.15$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8QsgJLhGrGm",
        "colab_type": "text"
      },
      "source": [
        "소프트 맥스 함수를 적용하니 각 클래스의 확률은 자동차 59%, 비행기 26%, 로켓 15%가 되었습니다.   \n",
        "그리고 확률의 합도 1이 되었습니다.     \n",
        "출력층의 강도가 정규화 된것입니다.   \n",
        "두번째 샘플에도 소프트 맥스 함수를 적용해보겠습니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHZWUyUgG9Z6",
        "colab_type": "text"
      },
      "source": [
        "##$z_{1}=-ln(\\frac{1}{0.5}-1)=0.0$ $z_{2}=-ln(\\frac{1}{0.2}-1)=-1.39$ $z_{3}=-ln(\\frac{1}{0.1}-1)=-2.20$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEOuTl50HOl_",
        "colab_type": "text"
      },
      "source": [
        "##$\\widehat{y_{1}}=\\frac{e^{0.0}}{e^{0.0}+e^{-1.39}+e^{-2.20}}=0.74$ $\\widehat{y_{2}}=\\frac{e^{-1.39}}{e^{0.0}+e^{-1.39}+e^{-2.20}}=0.18$$\\widehat{y_{3}}=\\frac{e^{-2.20}}{e^{0.0}+e^{-1.39}+e^{-2.20}}=0.08$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB9_YMiBHvb4",
        "colab_type": "text"
      },
      "source": [
        "두번째 샘플의 경우 자동차일 확률이 74%라고 예측 되었습니다.   \n",
        "처음에는 직관에 의존하여 '오른쪽 샘플이 왼쪽샘플보다 자동차일 가능성이 더 높다'라고 예측 했지만 이제는 확실히 말 할수 있게 되었습니다.  \n",
        "소프트 맥스 함수를 이용하여 확률의 합을 1로 만드니 비교하기 좋아졌습니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uy0Cw2_JHBv",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://github.com/JungMinNoh/jungminnoh/blob/master/7-6.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OnKThcvJLHP",
        "colab_type": "text"
      },
      "source": [
        "다중 분류에서 출력층을 통과한 값들은 소프트 맥스 함수를 거치며 적절한 확률값으로 변합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3IGT2aCJTDE",
        "colab_type": "text"
      },
      "source": [
        "## 크로스 엔트로피 손실함수를 도입합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geTZpMUCJY3h",
        "colab_type": "text"
      },
      "source": [
        "이제 이 확률값을 이용하여 가중치와 절편을 업데이트 하기 위한 손실함수를 알아보겠습니다.   \n",
        "다중분류에는 로지스틱 손실함수의 '일반화 버전'인 크로스 엔트로피 손실함수를 사용합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tUonrBqJokW",
        "colab_type": "text"
      },
      "source": [
        "크로스 엔트로피 손실함수를 소개할때 '로지스틱 손실 함수를 일반화 했다'라고 한 이유는   \n",
        "로지스틱 손실함수가 크로스 엔트로피 손실함수의 이진 분류 버전이기 때문입니다. 두 함수를 비교해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y4bLs08KWTk",
        "colab_type": "text"
      },
      "source": [
        "## 1.크로스 엔트로피 손실함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-0N8kh7Laww",
        "colab_type": "text"
      },
      "source": [
        "##$L=-\\sum_{c=1}^{c}y_{c}log(a_{c})=-(y_{1}log(a_{1})+y_{2}log(a_{2})\\cdots+y_{c}log(a_{c}))=-1\\times log(y_{a=1})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P62I-8UuLoPq",
        "colab_type": "text"
      },
      "source": [
        "## 2.로지스틱 손실 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAVpbCoOLuzd",
        "colab_type": "text"
      },
      "source": [
        "## $L = -(ylog(a)+(1-y)log(1-a))$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0Ia-ltsMRif",
        "colab_type": "text"
      },
      "source": [
        "크로스 엔트로피 손실 함수의 시그마 기호위의 값 (c) 은 전체 클래스 개수를 의미합니다.   \n",
        "앞에 예제에 적용하면 자동차,비행기,로켓으로 구분한 클래스 개수이므로 3이 됩니다.   \n",
        "분류 문제에서 정답 클래스의 타깃은 1이고 나머지 클래스의 타깃은 0입니다.   앞에서 본 예제중 하나에 적용 하면 -(1x0.74 + 0x0.18 + 0x0.08)입니다.  \n",
        " 결국 정답 클래스를 제외한 나머지 클래스 (y=0)에 대한 손실항은 모두 제거 되어"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd8XhNrkND7v",
        "colab_type": "text"
      },
      "source": [
        "##$L=-log(a_{y=1})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHy8ISy2NUTJ",
        "colab_type": "text"
      },
      "source": [
        "이 됩니다. a는 정답 클래스에 해당하는 뉴런의 활성화 출력을 말합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UulNM4ecNc9r",
        "colab_type": "text"
      },
      "source": [
        "##로지스틱 손실함수와 크로스 엔트로피 손실 함수는 매우 비슷합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKvk-TRCQRyG",
        "colab_type": "text"
      },
      "source": [
        "크로스 엔트로피 손실 함수와 로지스틱 손실 함수의 차이를 알아보기 위해   \n",
        "로지스틱 손실 함수를 타깃으로 양성 클래스 (y=1)인 경우와 음성 클래스(y=0)인 경우로 나눠 식을 정리해보겠습니다.   \n",
        "로지스틱 손실 함수는 양성클래스 일때 두번째 항이 소거 되고 음성 클래스 일때는 첫번째항이 소거 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2MJtHAOQsaU",
        "colab_type": "text"
      },
      "source": [
        "###$y = -log a$   양성 클래스인경우 y=1 \n",
        "###$y = -log(1-a)$ 음성 클래스인경우 y=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4aXMN5kRIdS",
        "colab_type": "text"
      },
      "source": [
        "($1-a$)를 음성 클래스의 활성화 출력이라 생각 하면 크로스 엔트로피 손실 함수를 정리한 식과 같다고 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq7QFZdRRjz5",
        "colab_type": "text"
      },
      "source": [
        "# 크로스 엔트로피 손실 함수를 미분합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDMaKKoYRziE",
        "colab_type": "text"
      },
      "source": [
        "경사 하강법을 사용하기 위해 크로스 엔트로피 손실 함수를 미분해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsEfDr3rR-nd",
        "colab_type": "text"
      },
      "source": [
        "자동차,비행기,로켓을 분류하는 3개의 클래스를 가진 다중분류의 예를 들어 크로스 엔트로피 손실 함수의 미분 과정을 알아 보겠습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur-jcua_SIjS",
        "colab_type": "text"
      },
      "source": [
        "###z1에 대해 미분합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qife6JrDSPcu",
        "colab_type": "text"
      },
      "source": [
        "먼저 z1에 대한 크로스 엔트로피 손실함수의 미분을 연쇄 법칙을 적용하여 나타냅니다. 연쇄법칙을 적용할때 한가지 주의할것이 있습니다. 시그모이드 함수와는 달리 소프트맥스 함수의 출력 a1,a2,a3 가 모두 z1의 함수이고 손실함수L이 a1,a2,a3의 함수 이므로 연쇄법칙은 다음과 같이 나타낼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oCNpxtiS8fp",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial L}{\\partial z_{1}}=\\frac{\\partial L}{\\partial a_{1}}\\frac{\\partial a_{1}}{\\partial z_{1}} + \\frac{\\partial L}{\\partial a_{2}}\\frac{\\partial a_{2}}{\\partial z_{1}} + \\frac{\\partial L}{\\partial a_{3}}\\frac{\\partial a_{3}}{\\partial z_{1}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzByyHm6TuEX",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://github.com/JungMinNoh/jungminnoh/blob/master/7-7.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4bvgRUqUhfS",
        "colab_type": "text"
      },
      "source": [
        "먼저 $\\frac{\\partial L}{\\partial a_{1}}$을 구하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYRP8sLBU172",
        "colab_type": "text"
      },
      "source": [
        "크로스 엔트로피 손실 함수는\n",
        "## $\\frac{\\partial L}{\\partial a_{1}}=-\\frac{\\partial }{\\partial a_{1}}(y_{1}log(a_{1})+y_{2}log(a_{2})+y_{3}log(a_{3}))=-\\frac{y_{1}}{a_{1}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW0faDm-WM9O",
        "colab_type": "text"
      },
      "source": [
        "$y_{2}log(a_{2})+y_{3}log(a_{3})$ 는 $a_{1}$의 함수가 아니므로 소거 되고 $y_{1}log(a_{1})$만 남습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m24J8qV2WoN4",
        "colab_type": "text"
      },
      "source": [
        "$log(a)$의 도함수는$\\frac{1}{a}$이므로 $-\\frac{y_{1}}{a_{1}}$이 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-qMSoNUXBjq",
        "colab_type": "text"
      },
      "source": [
        "마찬가지 방법으로 $a_{2}$,$a_{3}$에 대해 미분하면 다음과 같은 결과를 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq5KEB2WXUlH",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial L}{\\partial a_{2}}= -\\frac{y_{2}}{a_{2}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKpUEihVXg_o",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial L}{\\partial a_{3}}= -\\frac{y_{3}}{a_{3}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbjH6lvKXmep",
        "colab_type": "text"
      },
      "source": [
        "이 식을 $\\frac{\\partial L}{\\partial z_{1}}$ 에 대입하면 다음과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgL17xQXX29I",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial L}{\\partial z_{1}}=\\frac{\\partial L}{\\partial a_{1}}\\frac{\\partial a_{1}}{\\partial z_{1}} + \\frac{\\partial L}{\\partial a_{2}}\\frac{\\partial a_{2}}{\\partial z_{1}} + \\frac{\\partial L}{\\partial a_{3}}\\frac{\\partial a_{3}}{\\partial z_{1}}$ = ($-\\frac{y_{1}}{a_{1}})\\frac{\\partial a_{1}}{\\partial z_{1}} + (-\\frac{y_{2}}{a_{2}})\\frac{\\partial a_{2}}{\\partial z_{1}} + (-\\frac{y_{3}}{a_{3}})\\frac{\\partial a_{3}}{\\partial z_{1}}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU4oWQzGZRYc",
        "colab_type": "text"
      },
      "source": [
        "이제 $\\frac{\\partial a_{1}}{\\partial z_{1}}$ 에 대해 유도 해보겠습니다. 먼저 $ a_{1}$ 은 소프트 맥스 함수의 출력값이므로 이에 대한 식을 풀어서 써야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTv4ipEpUCKS",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial a_{1}}{\\partial z_{1}}= \\frac{\\partial }{\\partial z_{1}}(\\frac{e^{z_{1}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4j7yHPsakEM",
        "colab_type": "text"
      },
      "source": [
        "계산과정 생략 (수식 너무 많음)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zjacqaMamiT",
        "colab_type": "text"
      },
      "source": [
        "결론은"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuHvqIg9atOS",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial a_{1}}{\\partial z_{1}}=a_{1}(1-a_{1})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01QOm9EVbFDL",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial a_{2}}{\\partial z_{1}}=-a_{2}a_{1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux3PII0VbM03",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial a_{3}}{\\partial z_{1}}=-a_{3}a_{1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkQyQThpbRv8",
        "colab_type": "text"
      },
      "source": [
        "이 값들을 $\\frac{\\partial L}{\\partial z_{1}}$ 에 적용하면"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FNOyEBdbgWv",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial L}{\\partial z_{1}}=\\frac{\\partial L}{\\partial a_{1}}\\frac{\\partial a_{1}}{\\partial z_{1}} + \\frac{\\partial L}{\\partial a_{2}}\\frac{\\partial a_{2}}{\\partial z_{1}} + \\frac{\\partial L}{\\partial a_{3}}\\frac{\\partial a_{3}}{\\partial z_{1}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXflUls_bouM",
        "colab_type": "text"
      },
      "source": [
        "#$= (-\\frac{y_{1}}{a_{1}})\\frac{\\partial a_{1}}{\\partial z_{1}} + (-\\frac{y_{2}}{a_{2}})\\frac{\\partial a_{2}}{\\partial z_{1}} + (-\\frac{y_{3}}{a_{3}})\\frac{\\partial a_{3}}{\\partial z_{1}}$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhmUqqdgbsR1",
        "colab_type": "text"
      },
      "source": [
        "#$= (-\\frac{y_{1}}{a_{1}})a_{1}(1-a_{1}) + (-\\frac{y_{2}}{a_{2}})(-a_{2}a_{1}) + (-\\frac{y_{3}}{a_{3}})(-a_{3}a_{1})$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJA66U69cqNf",
        "colab_type": "text"
      },
      "source": [
        "#$= -y_{1}(1-a_{1})+y_{2}a_{1}+y_{3}a_{1} $ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzYGhe_KeA-o",
        "colab_type": "text"
      },
      "source": [
        "#$= -y_{1}+(y_{1}+y_{2}+y_{3})a_{1} $ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrE6JUqkd4Hv",
        "colab_type": "text"
      },
      "source": [
        "#$= -(y_{1}-a_{1}) $ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQK7b3U4ee00",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial L}{\\partial z_{1}} = -(y_{1}-a_{1}) $ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_L84qNMen5Z",
        "colab_type": "text"
      },
      "source": [
        "벡터 **$z$** 에 대해 정리 하면 크로스 엔트로피 손실함수의 미분 결과는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecAjjlvIev2I",
        "colab_type": "text"
      },
      "source": [
        "#$\\frac{\\partial L}{\\partial z_{}} = -(y_{}-a_{}) $ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16R0LJsoe4ts",
        "colab_type": "text"
      },
      "source": [
        "### 따라서 크로스 엔트로피 손실함수를 역전파에 사용하기 위해 코드를 따로 구현할 필요가 없습니다. 즉 4장에서 구현한 backward() 메서드를 그대로 사용할수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-cOQwBaf3LT",
        "colab_type": "text"
      },
      "source": [
        "# **다중분류 신경망을 구현합니다.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Itr1srsOgS2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsfTjl7Yr5SC",
        "colab_type": "text"
      },
      "source": [
        "이제 Minibatchnetwork 클래스를 확장하여 다중 분류를 수행할 수 있는 MultiClassNetwork 클래스를 구현해 보겠습니다.   \n",
        "앞에서 말했듯이 다중분류의 경사 하강법 알고리즘은 이진 분류의 경사 하강법 알고리즘과 원리는 같고 소프드 맥스함수가 추가 된점만 다르므로 이 부분만 수정하면 간단히 구현 할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBocWOGPsbEf",
        "colab_type": "text"
      },
      "source": [
        "##1.소프트 맥스 함수 추가하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImsLj0zosmGO",
        "colab_type": "text"
      },
      "source": [
        "지금까지 활성화 함수로 시그모이드 함수만 사용했습니다. 다중 분류에서는 마지막 출력 층에 소프트 맥스 함수를 사용해야 하므로 은닉층과 출력층에 각기 다른 활성화 함수를 적용 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAQ5oxfRsxKB",
        "colab_type": "text"
      },
      "source": [
        "이를 위해 activation()매서드의 이름을 sigmoid()로 바꾸고 softmax()메서드를 추가 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbwE0Ydps4O_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def sigmoid(self, z):\n",
        "        a = 1 / (1 + np.exp(-z))              # 시그모이드 계산\n",
        "        return a\n",
        "    \n",
        "    def softmax(self, z):\n",
        "        # 소프트맥스 함수\n",
        "        exp_z = np.exp(z)\n",
        "        return exp_z / np.sum(exp_z, axis=1).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP-rLZi6uvIp",
        "colab_type": "text"
      },
      "source": [
        "소프트 맥스 함수를 구현한 과정을 잠시 살펴 봅니다. 소프트 맥수 함수는 $\\frac{e^{z_{i}}}{e^{z_{1}}+e^{z_{2}}+e^{z_{3}}}$ 이므로 이 값을 계산하려면 먼저 $e^{z}$를 계산해야 합니다. np.exp(z)는 z의 각 원소를 $e^{z}$로 만들어 줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPXilcKQs_0Z",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://github.com/JungMinNoh/jungminnoh/blob/master/7-9.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhXIuM21v2Ya",
        "colab_type": "text"
      },
      "source": [
        "그런 다음 np.sum()함수를 사용하여 z의 각 행의 합을 계산합니다. 이때의 결과 형태는 행벡터(1차원 배열)이므로 다시 열 벡터 (2차원 배열) 로 바꿔야 나눗셈에 적용 할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKXa20PEuZJY",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://github.com/JungMinNoh/jungminnoh/blob/master/7-10.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6bA_xMEwHJ_",
        "colab_type": "text"
      },
      "source": [
        "마지막으로 나눗셈을 적용 하면 소프트 맥스 함수의 적용이 끝납니다. 나눗셈 결과의 각 행의 합이 1이므로 소프트 맥스 함수가 잘 적용 되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwKKk8s6udjB",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://github.com/JungMinNoh/jungminnoh/blob/master/7-11.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWbwkYPWwOwq",
        "colab_type": "text"
      },
      "source": [
        "##2. 정방향 계산하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DUomWU6wRc7",
        "colab_type": "text"
      },
      "source": [
        "activation()메서드의 이름을 sigmoid()로 바꿨으니 forward()메서드에 사용된 activation()메서드의 이름도 sigmoid()로 바꿔야 합니다. 나머지 코드는 동일 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ao_ih3xwdZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def forpass(self, x):\n",
        "        z1 = np.dot(x, self.w1) + self.b1        # 첫 번째 층의 선형 식을 계산합니다\n",
        "        self.a1 = self.sigmoid(z1)               # 활성화 함수를 적용합니다 # acivation() 이 sigmoid() 로 바뀜.\n",
        "        z2 = np.dot(self.a1, self.w2) + self.b2  # 두 번째 층의 선형 식을 계산합니다.\n",
        "        return z2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD_BXI4Bw3V5",
        "colab_type": "text"
      },
      "source": [
        "##3.가중치 초기화 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m8xnnVuw5J8",
        "colab_type": "text"
      },
      "source": [
        "이진 분류에서는 출력층의 뉴런이 1개 이므로 가중치 w2의 크기는 (은닉층의_뉴런_개수,1)로 지정 했습니다. 다중 분류도 마찬가지의 규칙을 따릅니다. 단 출력층의 뉴런이 2개 이상이므로 가중치 w2의 크기는 (은닉층의_뉴런_개수, 클래스_개수)가 됩니다. b2의 크기는 클래스 개수에 따라 지정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HvnUFS04-gY",
        "colab_type": "text"
      },
      "source": [
        "다음은 w2를 random.normal()함수를 이용하여 크기가 (은닉층의크기,클래스 개수)인 배열의 각 원소의 값을 정규분포를 따르는 무작위 수로 초기화 합니다. b2는 모두 0으로 초기화 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOMR-XB03Vk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(self, n_features, n_classes):\n",
        "        self.w1 = np.random.normal(0, 1, \n",
        "                                   (n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
        "        self.b1 = np.zeros(self.units)                        # 은닉층의 크기\n",
        "        self.w2 = np.random.normal(0, 1, \n",
        "                                   (self.units, n_classes))   # (은닉층의 크기, 클래스 개수)\n",
        "        self.b2 = np.zeros(n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePXRg4qVwv2a",
        "colab_type": "text"
      },
      "source": [
        "##4.fit메서드 수정하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh1h3RaW5Qh_",
        "colab_type": "text"
      },
      "source": [
        "fit()메서드는 몇 가지만 수정하면 됩니다.  \n",
        "가중치를 초기화 하는 init_weights()메서드를 호출할 때 클래스의개수를 매개변수의 값으로 넘겨줍니다.  \n",
        "다중 분류 문제에서 y,y_val은 2차원 행렬이므로 열 벡터로 변환하던 코드를 지웁니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLYTJnhq5orj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
        "        np.random.seed(42)\n",
        "        self.init_weights(x.shape[1], y.shape[1])    # 은닉층과 출력층의 가중치를 초기화합니다.\n",
        "        # epochs만큼 반복합니다.\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            print('.', end='')\n",
        "            # 제너레이터 함수에서 반환한 미니배치를 순환합니다.\n",
        "            for x_batch, y_batch in self.gen_batch(x, y):\n",
        "                a = self.training(x_batch, y_batch)\n",
        "                # 안전한 로그 계산을 위해 클리핑합니다.\n",
        "                a = np.clip(a, 1e-10, 1-1e-10)\n",
        "                # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
        "                loss += np.sum(-y_batch*np.log(a))\n",
        "            self.losses.append((loss + self.reg_loss()) / len(x))\n",
        "            # 검증 세트에 대한 손실을 계산합니다.\n",
        "            self.update_val_loss(x_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba8Y6Us356Tw",
        "colab_type": "text"
      },
      "source": [
        "## 5.training()메서드 수정하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nH__7FY5_Fe",
        "colab_type": "text"
      },
      "source": [
        "training()메서드에서 사용하는 출력층의 활성화 함수를 activation()메서드에서 softmax()메서드로 바꿉니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-3w6-A56HpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def training(self, x, y):\n",
        "        m = len(x)                # 샘플 개수를 저장합니다.\n",
        "        z = self.forpass(x)       # 정방향 계산을 수행합니다.\n",
        "        a = self.softmax(z)       # 활성화 함수를 적용합니다.\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NySWfVYq6MaF",
        "colab_type": "text"
      },
      "source": [
        "### 6.predict()메서드 수정하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TVVWoFz6S0J",
        "colab_type": "text"
      },
      "source": [
        "predict()메서드에서는 정방향계산에서 얻은 출력 중 가장 큰 값의 인덱스를 구합니다. 이 값이 예측 클래스가 됩니다.   \n",
        "이진 분류에서와 마찬가지로 클래스를 예측할때는 활성화 함수, 즉 소프트 맥스 함수를 거칠 필요가 없습니다.   \n",
        "출력층에서 계산된 선형 계산만으로 클래스를 예측 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHErBF3m6n4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(self, x):\n",
        "        z = self.forpass(x)          # 정방향 계산을 수행합니다.\n",
        "        return np.argmax(z, axis=1)  # 가장 큰 값의 인덱스를 반환합니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV_skfIz6lh3",
        "colab_type": "text"
      },
      "source": [
        "## 7. score()메서드 수정하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AojSt5O_6wyi",
        "colab_type": "text"
      },
      "source": [
        "score(0메서드에서는 predict()메서드의 결과와 타깃y의 클래스를 비교합니다.  \n",
        "이를 위해 배열 y의 행을 따라 가장 큰 값의 인덱스를 구해 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzIlf1Wv9FcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score(self, x, y):\n",
        "        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환합니다.\n",
        "        return np.mean(self.predict(x) == np.argmax(y, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cwF5JRe9LFY",
        "colab_type": "text"
      },
      "source": [
        "## 8.검증 손실 계산하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjScilRJ9OEK",
        "colab_type": "text"
      },
      "source": [
        "update_val_loss() 메서드에서 사용하는 활성화 함수를 softmax()로 바꿉니다.  \n",
        "또 로지스틱 손실 계산을 크로스 엔트로피 손실 계산으로 바꿉니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxxUwVX29fTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def update_val_loss(self, x_val, y_val):\n",
        "       \n",
        "        a = self.softmax(z)                # 활성화 함수를 적용합니다.\n",
        "        \n",
        "        # 크로스 엔트로피 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
        "        val_loss = np.sum(-y_val*np.log(a))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms7vlP_69otv",
        "colab_type": "text"
      },
      "source": [
        "#모두 수정한 식"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KARIbtmUbAiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiClassNetwork:\n",
        "    \n",
        "    def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n",
        "        self.units = units         # 은닉층의 뉴런 개수\n",
        "        self.batch_size = batch_size     # 배치 크기\n",
        "        self.w1 = None             # 은닉층의 가중치\n",
        "        self.b1 = None             # 은닉층의 절편\n",
        "        self.w2 = None             # 출력층의 가중치\n",
        "        self.b2 = None             # 출력층의 절편\n",
        "        self.a1 = None             # 은닉층의 활성화 출력\n",
        "        self.losses = []           # 훈련 손실\n",
        "        self.val_losses = []       # 검증 손실\n",
        "        self.lr = learning_rate    # 학습률\n",
        "        self.l1 = l1               # L1 손실 하이퍼파라미터\n",
        "        self.l2 = l2               # L2 손실 하이퍼파라미터\n",
        "\n",
        "    def forpass(self, x):\n",
        "        z1 = np.dot(x, self.w1) + self.b1        # 첫 번째 층의 선형 식을 계산합니다\n",
        "        self.a1 = self.sigmoid(z1)               # 활성화 함수를 적용합니다\n",
        "        z2 = np.dot(self.a1, self.w2) + self.b2  # 두 번째 층의 선형 식을 계산합니다.\n",
        "        return z2\n",
        "\n",
        "    def backprop(self, x, err):\n",
        "        m = len(x)       # 샘플 개수\n",
        "        # 출력층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
        "        w2_grad = np.dot(self.a1.T, err) / m\n",
        "        b2_grad = np.sum(err) / m\n",
        "        # 시그모이드 함수까지 그래디언트를 계산합니다.\n",
        "        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)\n",
        "        # 은닉층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
        "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
        "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
        "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
        "    \n",
        "    def sigmoid(self, z):\n",
        "        a = 1 / (1 + np.exp(-z))              # 시그모이드 계산\n",
        "        return a\n",
        "    \n",
        "    def softmax(self, z):\n",
        "        # 소프트맥스 함수\n",
        "        exp_z = np.exp(z)\n",
        "        return exp_z / np.sum(exp_z, axis=1).reshape(-1, 1)\n",
        " \n",
        "    def init_weights(self, n_features, n_classes):\n",
        "        self.w1 = np.random.normal(0, 1, \n",
        "                                   (n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
        "        self.b1 = np.zeros(self.units)                        # 은닉층의 크기\n",
        "        self.w2 = np.random.normal(0, 1, \n",
        "                                   (self.units, n_classes))   # (은닉층의 크기, 클래스 개수)\n",
        "        self.b2 = np.zeros(n_classes)\n",
        "        \n",
        "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
        "        np.random.seed(42)\n",
        "        self.init_weights(x.shape[1], y.shape[1])    # 은닉층과 출력층의 가중치를 초기화합니다.\n",
        "        # epochs만큼 반복합니다.\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            print('.', end='')\n",
        "            # 제너레이터 함수에서 반환한 미니배치를 순환합니다.\n",
        "            for x_batch, y_batch in self.gen_batch(x, y):\n",
        "                a = self.training(x_batch, y_batch)\n",
        "                # 안전한 로그 계산을 위해 클리핑합니다.\n",
        "                a = np.clip(a, 1e-10, 1-1e-10)\n",
        "                # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
        "                loss += np.sum(-y_batch*np.log(a))\n",
        "            self.losses.append((loss + self.reg_loss()) / len(x))\n",
        "            # 검증 세트에 대한 손실을 계산합니다.\n",
        "            self.update_val_loss(x_val, y_val)\n",
        "\n",
        "    # 미니배치 제너레이터 함수\n",
        "    def gen_batch(self, x, y):\n",
        "        length = len(x)\n",
        "        bins = length // self.batch_size # 미니배치 횟수\n",
        "        if length % self.batch_size:\n",
        "            bins += 1                    # 나누어 떨어지지 않을 때\n",
        "        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞습니다.\n",
        "        x = x[indexes]\n",
        "        y = y[indexes]\n",
        "        for i in range(bins):\n",
        "            start = self.batch_size * i\n",
        "            end = self.batch_size * (i + 1)\n",
        "            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환합니다.\n",
        "            \n",
        "    def training(self, x, y):\n",
        "        m = len(x)                # 샘플 개수를 저장합니다.\n",
        "        z = self.forpass(x)       # 정방향 계산을 수행합니다.\n",
        "        a = self.softmax(z)       # 활성화 함수를 적용합니다.\n",
        "        err = -(y - a)            # 오차를 계산합니다.\n",
        "        # 오차를 역전파하여 그래디언트를 계산합니다.\n",
        "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
        "        # 그래디언트에서 페널티 항의 미분 값을 뺍니다\n",
        "        w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m\n",
        "        w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m\n",
        "        # 은닉층의 가중치와 절편을 업데이트합니다.\n",
        "        self.w1 -= self.lr * w1_grad\n",
        "        self.b1 -= self.lr * b1_grad\n",
        "        # 출력층의 가중치와 절편을 업데이트합니다.\n",
        "        self.w2 -= self.lr * w2_grad\n",
        "        self.b2 -= self.lr * b2_grad\n",
        "        return a\n",
        "   \n",
        "    def predict(self, x):\n",
        "        z = self.forpass(x)          # 정방향 계산을 수행합니다.\n",
        "        return np.argmax(z, axis=1)  # 가장 큰 값의 인덱스를 반환합니다.\n",
        "    \n",
        "    def score(self, x, y):\n",
        "        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환합니다.\n",
        "        return np.mean(self.predict(x) == np.argmax(y, axis=1))\n",
        "\n",
        "    def reg_loss(self):\n",
        "        # 은닉층과 출력층의 가중치에 규제를 적용합니다.\n",
        "        return self.l1 * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + \\\n",
        "               self.l2 / 2 * (np.sum(self.w1**2) + np.sum(self.w2**2))\n",
        "\n",
        "    def update_val_loss(self, x_val, y_val):\n",
        "        z = self.forpass(x_val)            # 정방향 계산을 수행합니다.\n",
        "        a = self.softmax(z)                # 활성화 함수를 적용합니다.\n",
        "        a = np.clip(a, 1e-10, 1-1e-10)     # 출력 값을 클리핑합니다.\n",
        "        # 크로스 엔트로피 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
        "        val_loss = np.sum(-y_val*np.log(a))\n",
        "        self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}